{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2098eebb",
   "metadata": {},
   "source": [
    "# Capstone Project Overview\n",
    "\n",
    "In this lesson, participants will be introduced to the capstone project, including its objectives, requirements, and the design of the ETL pipeline. This foundational understanding will set the stage for successful project execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfc37e5",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "- Understand the project requirements.\n",
    "- Identify the AWS services to be used.\n",
    "- Plan the ETL pipeline design.\n",
    "- Outline the project timeline and deliverables.\n",
    "- Establish collaboration methods with peers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6504d08",
   "metadata": {},
   "source": [
    "## Why This Matters\n",
    "\n",
    "The capstone project serves as a culmination of the skills learned throughout the course, allowing participants to apply their knowledge in a practical setting. A well-designed ETL pipeline is crucial for efficient data processing and integration, impacting the overall success of data analytics projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1850d8b",
   "metadata": {},
   "source": [
    "### Capstone Project Overview\n",
    "\n",
    "The capstone project aims to integrate various data sources and apply transformation logic to derive meaningful insights. Understanding the project scope and expectations ensures that participants are aligned with the project's goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9eeb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Defining Project Goals\n",
    "# The project goals may include:\n",
    "project_goals = [\n",
    "    'Integrate data from multiple sources',\n",
    "    'Transform data for analytics',\n",
    "    'Load data into a data warehouse'\n",
    "]\n",
    "print('Project Goals:', project_goals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f02b104",
   "metadata": {},
   "source": [
    "## Micro-Exercise 1\n",
    "### Define Project Requirements\n",
    "\n",
    "In this micro-exercise, you will list the requirements for the capstone project.\n",
    "\n",
    "```python\n",
    "# List the requirements for the capstone project.\n",
    "requirements = [\n",
    "    'Data sources: Sales, Inventory, Customer',\n",
    "    'Transformation needs: Aggregation, Filtering',\n",
    "    'Output format: CSV, JSON'\n",
    "]\n",
    "print('Project Requirements:', requirements)\n",
    "```\n",
    "**Hint:** Consider data sources, transformation needs, and output formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13669600",
   "metadata": {},
   "outputs": [],
   "source": [
    "requirements = [\n",
    "    'Data sources: Sales, Inventory, Customer',\n",
    "    'Transformation needs: Aggregation, Filtering',\n",
    "    'Output format: CSV, JSON'\n",
    "]\n",
    "print('Project Requirements:', requirements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b963fd",
   "metadata": {},
   "source": [
    "### ETL Pipeline Design\n",
    "\n",
    "The ETL (Extract, Transform, Load) pipeline is a critical component in data integration, enabling the movement and transformation of data from various sources to destinations. A well-designed ETL pipeline is crucial for efficient data processing and integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4384b79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: ETL Pipeline Components\n",
    "# Sample code for an ETL pipeline structure\n",
    "class ETLPipeline:\n",
    "    def extract(self):\n",
    "        # Code to extract data from source\n",
    "        pass\n",
    "    def transform(self):\n",
    "        # Code to transform data\n",
    "        pass\n",
    "    def load(self):\n",
    "        # Code to load data into destination\n",
    "        pass\n",
    "\n",
    "pipeline = ETLPipeline()\n",
    "print('ETL Pipeline created:', pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d066b729",
   "metadata": {},
   "source": [
    "## Micro-Exercise 2\n",
    "### Identify AWS Services\n",
    "\n",
    "In this micro-exercise, you will identify which AWS services will be used in the project.\n",
    "\n",
    "```python\n",
    "# Identify which AWS services will be used in the project.\n",
    "aws_services = [\n",
    "    'AWS Glue',\n",
    "    'Amazon S3',\n",
    "    'Amazon Redshift'\n",
    "]\n",
    "print('AWS Services:', aws_services)\n",
    "```\n",
    "**Hint:** Think about services like AWS Glue, S3, and Redshift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42eecf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_services = [\n",
    "    'AWS Glue',\n",
    "    'Amazon S3',\n",
    "    'Amazon Redshift'\n",
    "]\n",
    "print('AWS Services:', aws_services)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cb825d",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "### Example 1: Retail Analytics Project\n",
    "This example demonstrates how to build a complete data pipeline for a retail analytics project, showcasing the integration of various data sources.\n",
    "\n",
    "```python\n",
    "# Example code for data extraction from retail sales database.\n",
    "retail_data = extract_retail_data()\n",
    "print('Retail Data Extracted:', retail_data)\n",
    "```\n",
    "\n",
    "### Example 2: Healthcare Data Integration\n",
    "This example illustrates the design of an ETL pipeline for integrating healthcare data from multiple sources to improve patient care analytics.\n",
    "\n",
    "```python\n",
    "# Example code for transforming healthcare data.\n",
    "transformed_healthcare_data = transform_healthcare_data(healthcare_data)\n",
    "print('Transformed Healthcare Data:', transformed_healthcare_data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac57d765",
   "metadata": {},
   "source": [
    "## Main Exercise\n",
    "### Capstone Project Planning\n",
    "Participants will create a comprehensive project plan for the capstone project, detailing goals, stakeholders, and a timeline for milestones.\n",
    "\n",
    "```python\n",
    "# Draft a project plan document outlining goals, stakeholders, and timeline.\n",
    "project_plan = {\n",
    "    'goals': project_goals,\n",
    "    'stakeholders': ['Data Engineer', 'Data Analyst', 'Project Manager'],\n",
    "    'timeline': '4 weeks'\n",
    "}\n",
    "print('Project Plan:', project_plan)\n",
    "```\n",
    "**Expected Outcomes:**\n",
    "- A clear project plan document.\n",
    "- Identification of key stakeholders and their roles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81886a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_plan = {\n",
    "    'goals': project_goals,\n",
    "    'stakeholders': ['Data Engineer', 'Data Analyst', 'Project Manager'],\n",
    "    'timeline': '4 weeks'\n",
    "}\n",
    "print('Project Plan:', project_plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe2b105",
   "metadata": {},
   "source": [
    "## Common Mistakes\n",
    "- Not thoroughly planning the project, leading to scope creep.\n",
    "- Failing to identify all necessary AWS services, resulting in incomplete project execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34be37ac",
   "metadata": {},
   "source": [
    "## Recap\n",
    "In this lesson, we covered the capstone project overview, including its objectives, requirements, and the design of the ETL pipeline. As we move forward, participants will begin planning their projects and collaborating with peers to ensure successful execution."
   ]
  }
 ],
 "metadata": {
  "doc_links_used": [],
  "estimated_minutes": 45,
  "tags": [
   "intermediate",
   "AWS Glue",
   "ETL",
   "data integration"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
