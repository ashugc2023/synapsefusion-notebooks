{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73e69d59",
   "metadata": {},
   "source": [
    "# Data Ingestion\n",
    "\n",
    "In this lesson, you will learn how to ingest data into SageMaker from various sources, including Amazon S3, public datasets, and custom data sources. Understanding how to effectively bring data into SageMaker is the first step in the data preparation process.\n",
    "\n",
    "## Learning Objectives\n",
    "- Identify various data sources compatible with SageMaker.\n",
    "- Ingest data into SageMaker from Amazon S3.\n",
    "- Understand different data formats and their implications.\n",
    "- Explore public datasets available for machine learning.\n",
    "- Learn about custom data sources and ingestion methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a3a8f7",
   "metadata": {},
   "source": [
    "## Why This Matters\n",
    "\n",
    "Knowing where to find and how to access data is essential for any machine learning project. Data ingestion is the first step in the data preparation process, which is crucial for building effective machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8b07fc",
   "metadata": {},
   "source": [
    "### Data Sources\n",
    "\n",
    "Data sources are the origins from which data is obtained for machine learning models. In SageMaker, common data sources include Amazon S3, public datasets, and custom APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7899692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of accessing data from Amazon S3\n",
    "import boto3\n",
    "\n",
    "# Initialize a session using Amazon S3\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# List all buckets\n",
    "buckets = s3.list_buckets()\n",
    "for bucket in buckets['Buckets']:\n",
    "    print(bucket['Name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce29db1",
   "metadata": {},
   "source": [
    "#### Micro-Exercise 1: Identify Data Sources\n",
    "List at least three different data sources that can be used in SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e09ad5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data sources\n",
    "# 1. Amazon S3\n",
    "# 2. Public datasets\n",
    "# 3. Custom APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bd8021",
   "metadata": {},
   "source": [
    "### Data Formats\n",
    "\n",
    "Data formats refer to the structure in which data is stored. Common formats include CSV, JSON, and Parquet. Each format has its own advantages and disadvantages regarding processing speed and compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bbfc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of reading a CSV file in SageMaker\n",
    "import pandas as pd\n",
    "\n",
    "# Load a CSV file from S3\n",
    "csv_file_path = 's3://my-bucket/my-data.csv'\n",
    "data = pd.read_csv(csv_file_path)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae7c87c",
   "metadata": {},
   "source": [
    "#### Micro-Exercise 2: Describe Data Ingestion Steps\n",
    "Describe the steps to ingest data from an S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2029b0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps to ingest data\n",
    "# 1. Create an S3 bucket\n",
    "# 2. Upload data\n",
    "# 3. Use SageMaker to access the bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243edb15",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "### Example 1: Ingesting Data from S3\n",
    "This example demonstrates how to create an S3 bucket, upload a dataset, and access it from SageMaker.\n",
    "\n",
    "```bash\n",
    "# Create an S3 bucket\n",
    "aws s3 mb s3://my-bucket\n",
    "\n",
    "# Upload data\n",
    "aws s3 cp local-file.csv s3://my-bucket/\n",
    "```\n",
    "\n",
    "### Example 2: Using Public Datasets\n",
    "This example shows how to access a public dataset from a repository and ingest it into SageMaker.\n",
    "\n",
    "```python\n",
    "# Access public dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset_url = 'https://example.com/public-dataset.csv'\n",
    "dataset = pd.read_csv(dataset_url)\n",
    "print(dataset.head())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfb3c78",
   "metadata": {},
   "source": [
    "## Micro-Exercises\n",
    "\n",
    "1. **List at least three different data sources that can be used in SageMaker.**\n",
    "   \n",
    "   ```python\n",
    "   # Example data sources\n",
    "   # 1. Amazon S3\n",
    "   # 2. Public datasets\n",
    "   # 3. Custom APIs\n",
    "   ```\n",
    "\n",
    "2. **Describe the steps to ingest data from an S3 bucket.**\n",
    "   \n",
    "   ```python\n",
    "   # Steps to ingest data\n",
    "   # 1. Create an S3 bucket\n",
    "   # 2. Upload data\n",
    "   # 3. Use SageMaker to access the bucket\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9695b08d",
   "metadata": {},
   "source": [
    "## Main Exercise\n",
    "In this exercise, you will create an S3 bucket, upload a dataset, and also access a public dataset to ingest into SageMaker. You will verify the successful ingestion of both datasets.\n",
    "\n",
    "### Steps:\n",
    "1. Create an S3 bucket:\n",
    "   ```bash\n",
    "   aws s3 mb s3://my-bucket\n",
    "   ```\n",
    "2. Upload your local dataset:\n",
    "   ```bash\n",
    "   aws s3 cp local-file.csv s3://my-bucket/\n",
    "   ```\n",
    "3. Access a public dataset:\n",
    "   ```python\n",
    "   import pandas as pd\n",
    "   dataset_url = 'https://example.com/public-dataset.csv'\n",
    "   public_data = pd.read_csv(dataset_url)\n",
    "   print(public_data.head())\n",
    "   ```\n",
    "\n",
    "### Expected Outcomes:\n",
    "- The dataset from S3 is successfully ingested into SageMaker.\n",
    "- The public dataset is successfully accessed and ingested into SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972ad244",
   "metadata": {},
   "source": [
    "## Common Mistakes\n",
    "- Not verifying data integrity before ingestion.\n",
    "- Ignoring data format compatibility which can lead to errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a12b50",
   "metadata": {},
   "source": [
    "## Recap\n",
    "In this lesson, you learned about various data sources and formats compatible with SageMaker. You practiced ingesting data from Amazon S3 and public datasets. In the next lesson, we will explore data preprocessing techniques to prepare the ingested data for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b667eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional code cell for practice\n",
    "# Example of listing objects in an S3 bucket\n",
    "import boto3\n",
    "\n",
    "# Initialize a session using Amazon S3\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Specify the bucket name\n",
    "bucket_name = 'my-bucket'\n",
    "\n",
    "# List objects in the specified S3 bucket\n",
    "objects = s3.list_objects_v2(Bucket=bucket_name)\n",
    "if 'Contents' in objects:\n",
    "    for obj in objects['Contents']:\n",
    "        print(obj['Key'])\n",
    "else:\n",
    "    print('No objects found in the bucket.')"
   ]
  }
 ],
 "metadata": {
  "doc_links_used": [],
  "estimated_minutes": 45,
  "tags": [
   "beginner",
   "python"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
