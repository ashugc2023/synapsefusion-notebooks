{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b3b1d1e",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preprocessing\n",
    "\n",
    "In this lesson, you will learn about the essential techniques for cleaning and preprocessing data in AWS SageMaker. By the end of this lesson, you will be able to identify common data issues, apply cleaning techniques, and preprocess data for machine learning model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f6c857",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "- Identify common data issues that can affect model performance.\n",
    "- Apply cleaning techniques to handle missing values and outliers.\n",
    "- Preprocess data for training using feature scaling methods.\n",
    "- Understand the importance of data quality in machine learning.\n",
    "- Explore different preprocessing techniques available in SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c64da6f",
   "metadata": {},
   "source": [
    "## Why This Matters\n",
    "\n",
    "Data cleaning and preprocessing are crucial steps in the machine learning pipeline. High-quality data leads to better model performance, while poor-quality data can result in inaccurate predictions and unreliable models. Ensuring that your dataset is clean and well-prepared is essential for building effective machine learning solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a9fea0",
   "metadata": {},
   "source": [
    "## Concept: Data Cleaning\n",
    "\n",
    "Data cleaning involves identifying and correcting errors or inconsistencies in the data to improve its quality. This process is vital as it ensures that the dataset is accurate and reliable, which is crucial for building effective models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f69467e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code to handle missing values\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('dataset.csv')  # Load dataset\n",
    "# Identify and fill missing values\n",
    "# Using forward fill method\n",
    "print('Before filling missing values:')\n",
    "print(df.isnull().sum())\n",
    "df.fillna(method='ffill', inplace=True)\n",
    "print('After filling missing values:')\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9a2854",
   "metadata": {},
   "source": [
    "### Micro-Exercise: Identify Common Data Issues\n",
    "\n",
    "List common data issues that can affect model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4a65a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common data issues\n",
    "# 1. Missing values\n",
    "# 2. Outliers\n",
    "# 3. Incorrect data types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e14c9a",
   "metadata": {},
   "source": [
    "## Concept: Feature Scaling\n",
    "\n",
    "Feature scaling is the process of normalizing or standardizing the range of independent variables or features of data. This is important because it ensures that all features contribute equally to the model's performance, preventing bias towards certain features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2991b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code to apply feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# Assume df is the DataFrame with features to scale\n",
    "scaled_features = scaler.fit_transform(df)\n",
    "print('Scaled features:')\n",
    "print(scaled_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565196eb",
   "metadata": {},
   "source": [
    "### Micro-Exercise: Demonstrate Feature Scaling\n",
    "\n",
    "Demonstrate how to apply feature scaling to a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ad8d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code to apply feature scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "# Assume df is the DataFrame with features to scale\n",
    "scaled_features = scaler.fit_transform(df)\n",
    "print('Scaled features using MinMaxScaler:')\n",
    "print(scaled_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352944f7",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "### Example 1: Handling Missing Values\n",
    "This example demonstrates how to identify and fill missing values in a dataset using the forward fill method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1769c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code to handle missing values\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('dataset.csv')  # Load dataset\n",
    "# Identify and fill missing values\n",
    "print('Before filling missing values:')\n",
    "print(df.isnull().sum())\n",
    "df.fillna(method='ffill', inplace=True)\n",
    "print('After filling missing values:')\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5dfeba",
   "metadata": {},
   "source": [
    "### Example 2: Outlier Detection\n",
    "This example shows how to identify outliers using the IQR method and remove them from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9393a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code to remove outliers\n",
    "Q1 = df.quantile(0.25)\n",
    "Q3 = df.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "# Remove outliers\n",
    "df = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "print('Data after removing outliers:')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3baa85",
   "metadata": {},
   "source": [
    "## Micro-Exercises\n",
    "\n",
    "### Exercise 1: List Common Data Issues\n",
    "List common data issues that can affect model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe076c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common data issues\n",
    "# 1. Missing values\n",
    "# 2. Outliers\n",
    "# 3. Incorrect data types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4dcf1f",
   "metadata": {},
   "source": [
    "### Exercise 2: Handle Outliers\n",
    "Demonstrate how to handle outliers in a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45593f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code to handle outliers\n",
    "Q1 = df.quantile(0.25)\n",
    "Q3 = df.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "# Remove outliers\n",
    "df = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7743bb29",
   "metadata": {},
   "source": [
    "## Main Exercise: Cleaning and Preprocessing a Dataset\n",
    "In this exercise, you will load a dataset, identify and handle missing values, detect and remove outliers, and apply feature scaling to prepare the dataset for training a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4549d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "# Handle missing values\n",
    "print('Before filling missing values:')\n",
    "print(df.isnull().sum())\n",
    "df.fillna(method='ffill', inplace=True)\n",
    "print('After filling missing values:')\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Remove outliers\n",
    "Q1 = df.quantile(0.25)\n",
    "Q3 = df.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "# Remove outliers\n",
    "df = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "\n",
    "# Apply feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(df)\n",
    "print('Scaled features:')\n",
    "print(scaled_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d089c833",
   "metadata": {},
   "source": [
    "## Common Mistakes\n",
    "- Ignoring missing values which can skew model results.\n",
    "- Not normalizing data leading to poor model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558e4e3d",
   "metadata": {},
   "source": [
    "## Recap & Next Steps\n",
    "In this lesson, we covered the importance of data cleaning and preprocessing in machine learning. You learned how to identify and handle missing values, detect outliers, and apply feature scaling techniques. In the next lesson, we will explore model training and evaluation in AWS SageMaker."
   ]
  }
 ],
 "metadata": {
  "doc_links_used": [],
  "estimated_minutes": 45,
  "tags": [
   "beginner",
   "python",
   "data cleaning",
   "feature scaling"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
