{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f460781",
   "metadata": {},
   "source": [
    "# Monitoring Deployed Models\n",
    "\n",
    "In this lesson, we will explore the importance of monitoring deployed models in AWS SageMaker. By the end of this lesson, you will be able to:\n",
    "\n",
    "- Understand the importance of monitoring deployed models.\n",
    "- Implement monitoring strategies using SageMaker tools.\n",
    "- Evaluate model performance after deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab081f7",
   "metadata": {},
   "source": [
    "## Why This Matters\n",
    "\n",
    "Monitoring deployed models is crucial for ensuring their reliability and effectiveness. As models operate in real-world environments, they may encounter changes in data patterns, leading to performance degradation. By utilizing monitoring tools, you can track model performance, detect issues early, and maintain the accuracy of your machine learning solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1143a37",
   "metadata": {},
   "source": [
    "## Monitoring Tools\n",
    "\n",
    "Monitoring tools such as Amazon CloudWatch and SageMaker Model Monitor are essential for tracking the performance of deployed models. They provide insights into how models are functioning in real-time and help identify any issues that may arise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2268aad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code to configure CloudWatch\n",
    "import boto3\n",
    "\n",
    "# Create a CloudWatch client\n",
    "cloudwatch = boto3.client('cloudwatch')\n",
    "\n",
    "# Set up a metric alarm for model accuracy\n",
    "cloudwatch.put_metric_alarm(\n",
    "    AlarmName='ModelAccuracyAlarm',\n",
    "    MetricName='ModelAccuracy',\n",
    "    Namespace='SageMaker',\n",
    "    Statistic='Average',\n",
    "    Period=300,\n",
    "    Threshold=0.8,\n",
    "    ComparisonOperator='LessThanThreshold',\n",
    "    EvaluationPeriods=1,\n",
    "    AlarmActions=['arn:aws:sns:us-east-1:123456789012:MyTopic'],\n",
    "    Dimensions=[{'Name': 'EndpointName', 'Value': 'my-endpoint'}]\n",
    ")\n",
    "\n",
    "print('CloudWatch alarm configured successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cc4fdc",
   "metadata": {},
   "source": [
    "## Micro-Exercise 1\n",
    "\n",
    "### Task Description\n",
    "List at least two monitoring tools available in SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cd8b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starter code for Micro-Exercise 1\n",
    "# You can list the tools here\n",
    "monitoring_tools = ['Amazon CloudWatch', 'SageMaker Model Monitor']\n",
    "print('Monitoring Tools:', monitoring_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ec318d",
   "metadata": {},
   "source": [
    "## Model Performance Evaluation\n",
    "\n",
    "Evaluating model performance involves analyzing various metrics to determine how well a model is performing after deployment. This includes checking for model drift, which occurs when the statistical properties of the target variable change over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fd270d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code to set up Model Monitor\n",
    "from sagemaker.model_monitor import ModelMonitor\n",
    "\n",
    "# Create a Model Monitor instance\n",
    "monitor = ModelMonitor(\n",
    "    role='SageMakerRole',\n",
    "    endpoint_name='my-endpoint',\n",
    "    output_s3_uri='s3://my-bucket/monitoring/',\n",
    ")\n",
    "\n",
    "# Create a monitoring schedule for data quality\n",
    "monitor.create_monitoring_schedule(\n",
    "    monitoring_schedule_name='MyMonitoringSchedule',\n",
    "    monitoring_type='DataQuality',\n",
    "    monitoring_output_config={'S3OutputPath': 's3://my-bucket/monitoring/'},\n",
    "    monitoring_parameters={'MonitoringInterval': 'PT5M'}\n",
    ")\n",
    "\n",
    "print('Model Monitor set up successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eb0a4c",
   "metadata": {},
   "source": [
    "## Micro-Exercise 2\n",
    "\n",
    "### Task Description\n",
    "Describe how to evaluate the performance of a deployed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b5ce02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starter code for Micro-Exercise 2\n",
    "# You can describe the evaluation process here\n",
    "evaluation_process = '''\n",
    "1. Collect predictions from the model.\n",
    "2. Compare predictions with actual outcomes.\n",
    "3. Calculate metrics such as accuracy, precision, and recall.\n",
    "4. Monitor for model drift over time.\n",
    "''' \n",
    "print('Evaluation Process:', evaluation_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9d7cce",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "### Example 1: Setting Up Amazon CloudWatch for Monitoring\n",
    "This example demonstrates how to configure Amazon CloudWatch to monitor the performance of a deployed model, including setting up alerts for specific metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32ad2f5",
   "metadata": {},
   "source": [
    "### Example 2: Using SageMaker Model Monitor for Drift Detection\n",
    "This example illustrates how to set up SageMaker Model Monitor to detect model drift and evaluate the performance of the deployed model over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67bc189",
   "metadata": {},
   "source": [
    "## Micro-Exercises\n",
    "\n",
    "1. List at least two monitoring tools available in SageMaker.\n",
    "2. Describe how to evaluate the performance of a deployed model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7c070f",
   "metadata": {},
   "source": [
    "## Main Exercise\n",
    "\n",
    "### Task Description\n",
    "In this exercise, you will configure Amazon CloudWatch for your deployed model, set up alerts for key performance metrics, and review monitoring logs to adjust model parameters as needed.\n",
    "\n",
    "### Starter Code\n",
    "```python\n",
    "import boto3\n",
    "# Add your CloudWatch and Model Monitor setup code here\n",
    "```\n",
    "### Expected Outcomes\n",
    "- A monitoring setup that tracks the performance of the deployed model.\n",
    "- Alerts configured for any performance issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f513a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main exercise starter code\n",
    "import boto3\n",
    "from sagemaker.model_monitor import ModelMonitor\n",
    "\n",
    "# Create a CloudWatch client\n",
    "cloudwatch = boto3.client('cloudwatch')\n",
    "\n",
    "# Create a Model Monitor instance\n",
    "monitor = ModelMonitor(\n",
    "    role='SageMakerRole',\n",
    "    endpoint_name='my-endpoint',\n",
    "    output_s3_uri='s3://my-bucket/monitoring/',\n",
    ")\n",
    "\n",
    "# Set up a metric alarm for model accuracy\n",
    "cloudwatch.put_metric_alarm(\n",
    "    AlarmName='ModelAccuracyAlarm',\n",
    "    MetricName='ModelAccuracy',\n",
    "    Namespace='SageMaker',\n",
    "    Statistic='Average',\n",
    "    Period=300,\n",
    "    Threshold=0.8,\n",
    "    ComparisonOperator='LessThanThreshold',\n",
    "    EvaluationPeriods=1,\n",
    "    AlarmActions=['arn:aws:sns:us-east-1:123456789012:MyTopic'],\n",
    "    Dimensions=[{'Name': 'EndpointName', 'Value': 'my-endpoint'}]\n",
    ")\n",
    "\n",
    "print('CloudWatch alarm configured successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4259b2",
   "metadata": {},
   "source": [
    "## Common Mistakes\n",
    "- Neglecting to monitor models after deployment.\n",
    "- Ignoring signs of performance degradation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9820418",
   "metadata": {},
   "source": [
    "## Recap\n",
    "In this lesson, we covered the importance of monitoring deployed models and explored various tools available in AWS SageMaker. As you continue your journey in machine learning, remember to implement monitoring strategies to ensure the reliability and effectiveness of your models. In the next lesson, we will delve into advanced model optimization techniques."
   ]
  }
 ],
 "metadata": {
  "doc_links_used": [],
  "estimated_minutes": 45,
  "tags": [
   "beginner",
   "python"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
